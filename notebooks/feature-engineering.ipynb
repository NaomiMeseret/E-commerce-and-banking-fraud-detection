{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Data Transformation\n",
    "\n",
    "This notebook performs feature engineering and data transformation for both e-commerce and credit card fraud datasets.\n",
    "\n",
    "## Objectives\n",
    "1. Feature Engineering for Fraud_Data.csv:\n",
    "   - Transaction frequency and velocity features\n",
    "   - Time-based features (hour_of_day, day_of_week, time_since_signup)\n",
    "2. Data Transformation:\n",
    "   - Normalize/scale numerical features\n",
    "   - Encode categorical features\n",
    "3. Handle Class Imbalance:\n",
    "   - Apply SMOTE or undersampling to training data only\n",
    "   - Document class distribution before and after resampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import ipaddress\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths - ensure we're working from the project root\n",
    "project_root = Path().resolve()\n",
    "if project_root.name == 'notebooks':\n",
    "    project_root = project_root.parent\n",
    "elif (project_root / 'notebooks').exists():\n",
    "    pass  # Already at project root\n",
    "else:\n",
    "    # Try to find project root by looking for data directory\n",
    "    current = Path().resolve()\n",
    "    while current != current.parent:\n",
    "        if (current / 'data').exists():\n",
    "            project_root = current\n",
    "            break\n",
    "        current = current.parent\n",
    "\n",
    "DATA_DIR = project_root / 'data' / 'raw'\n",
    "PROCESSED_DIR = project_root / 'data' / 'processed'\n",
    "OUTPUT_DIR = project_root / 'outputs' / 'eda' / 'feature-engineering'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # Ensure output directory exists\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)  # Ensure processed directory exists\n",
    "\n",
    "# Print relative paths so output is the same for all team members\n",
    "print(f\"Project root: .\")\n",
    "print(f\"Data directory: {DATA_DIR.relative_to(project_root)}\")\n",
    "print(f\"Processed directory: {PROCESSED_DIR.relative_to(project_root)}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR.relative_to(project_root)}\")\n",
    "print(f\"Data directory exists: {DATA_DIR.exists()}\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Helper function to save plots (prints relative path for portability)\n",
    "def save_plot(fig, filename, dpi=300, bbox_inches='tight'):\n",
    "    \"\"\"Save plot to output directory\"\"\"\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    fig.savefig(filepath, dpi=dpi, bbox_inches=bbox_inches)\n",
    "    rel = filepath.relative_to(project_root)\n",
    "    print(f\"Plot saved to: {rel}\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: E-commerce Fraud Data Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned fraud data (assuming EDA notebook has been run)\n",
    "# If running independently, load and clean data here\n",
    "fraud_df = pd.read_csv(DATA_DIR / 'Fraud_Data.csv')\n",
    "\n",
    "# Convert timestamps\n",
    "fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time'])\n",
    "fraud_df['purchase_time'] = pd.to_datetime(fraud_df['purchase_time'])\n",
    "\n",
    "# Load IP country mapping if needed\n",
    "ip_country_df = pd.read_csv(DATA_DIR / 'IpAddress_to_Country.csv')\n",
    "\n",
    "print(f\"Fraud Data Shape: {fraud_df.shape}\")\n",
    "print(f\"Columns: {fraud_df.columns.tolist()}\")\n",
    "fraud_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Time-based Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour of day from purchase_time\n",
    "fraud_df['hour_of_day'] = fraud_df['purchase_time'].dt.hour\n",
    "\n",
    "# Extract day of week (0=Monday, 6=Sunday)\n",
    "fraud_df['day_of_week'] = fraud_df['purchase_time'].dt.dayofweek\n",
    "\n",
    "# Calculate time since signup (in hours)\n",
    "fraud_df['time_since_signup'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).dt.total_seconds() / 3600\n",
    "\n",
    "# Handle negative time_since_signup (purchase before signup - data error)\n",
    "fraud_df['time_since_signup'] = fraud_df['time_since_signup'].clip(lower=0)\n",
    "\n",
    "print(\"Time-based features created:\")\n",
    "print(fraud_df[['purchase_time', 'signup_time', 'hour_of_day', 'day_of_week', 'time_since_signup']].head(10))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nTime-based Features Summary:\")\n",
    "print(fraud_df[['hour_of_day', 'day_of_week', 'time_since_signup']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time-based features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Hour of day distribution\n",
    "hour_counts = fraud_df['hour_of_day'].value_counts().sort_index()\n",
    "axes[0].bar(hour_counts.index, hour_counts.values, color='steelblue')\n",
    "axes[0].set_title('Transactions by Hour of Day')\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# Day of week distribution\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "day_counts = fraud_df['day_of_week'].value_counts().sort_index()\n",
    "axes[1].bar(day_counts.index, day_counts.values, color='coral')\n",
    "axes[1].set_title('Transactions by Day of Week')\n",
    "axes[1].set_xlabel('Day of Week')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels(day_names)\n",
    "\n",
    "# Time since signup distribution\n",
    "axes[2].hist(fraud_df['time_since_signup'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[2].set_title('Time Since Signup Distribution')\n",
    "axes[2].set_xlabel('Hours Since Signup')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transaction Frequency and Velocity Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by user_id and purchase_time for proper calculation\n",
    "fraud_df = fraud_df.sort_values(['user_id', 'purchase_time']).reset_index(drop=True)\n",
    "\n",
    "# Calculate transaction frequency per user (total count)\n",
    "user_transaction_count = fraud_df.groupby('user_id').size().reset_index(name='total_transactions')\n",
    "fraud_df = fraud_df.merge(user_transaction_count, on='user_id', how='left')\n",
    "\n",
    "# OPTIMIZED: Calculate transaction frequency in time windows using vectorized operations\n",
    "# This uses merge_asof and groupby for much better performance\n",
    "print(\"Calculating transaction frequency features (optimized version)...\")\n",
    "\n",
    "# Convert purchase_time to numeric for faster comparisons (seconds since epoch)\n",
    "fraud_df['purchase_time_sec'] = (fraud_df['purchase_time'] - pd.Timestamp('1970-01-01')).dt.total_seconds()\n",
    "\n",
    "# Initialize columns\n",
    "fraud_df['transactions_last_24h'] = 0\n",
    "fraud_df['transactions_last_7d'] = 0\n",
    "fraud_df['transactions_last_30d'] = 0\n",
    "\n",
    "# Use groupby and vectorized operations - much faster!\n",
    "def calculate_window_counts(group):\n",
    "    \"\"\"Calculate rolling window counts for a user group\"\"\"\n",
    "    times = group['purchase_time_sec'].values\n",
    "    n = len(times)\n",
    "    counts_24h = []\n",
    "    counts_7d = []\n",
    "    counts_30d = []\n",
    "    \n",
    "    # Vectorized calculation using numpy broadcasting\n",
    "    for i in range(n):\n",
    "        current_time = times[i]\n",
    "        time_diffs = current_time - times[:i+1]  # Only look at past transactions\n",
    "        \n",
    "        counts_24h.append(np.sum(time_diffs <= 24*3600))  # 24 hours in seconds\n",
    "        counts_7d.append(np.sum(time_diffs <= 7*24*3600))  # 7 days in seconds\n",
    "        counts_30d.append(np.sum(time_diffs <= 30*24*3600))  # 30 days in seconds\n",
    "    \n",
    "    group['transactions_last_24h'] = counts_24h\n",
    "    group['transactions_last_7d'] = counts_7d\n",
    "    group['transactions_last_30d'] = counts_30d\n",
    "    return group\n",
    "\n",
    "# Apply to each user group\n",
    "fraud_df = fraud_df.groupby('user_id', group_keys=False).apply(calculate_window_counts)\n",
    "\n",
    "# Drop temporary column\n",
    "fraud_df = fraud_df.drop('purchase_time_sec', axis=1)\n",
    "\n",
    "print(\"✓ Transaction frequency features created!\")\n",
    "print(fraud_df[['user_id', 'purchase_time', 'total_transactions', \n",
    "                'transactions_last_24h', 'transactions_last_7d', 'transactions_last_30d']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transaction velocity (transactions per hour in last 24h)\n",
    "fraud_df['velocity_last_24h'] = fraud_df['transactions_last_24h'] / 24.0\n",
    "\n",
    "# Calculate average purchase value per user\n",
    "user_avg_purchase = fraud_df.groupby('user_id')['purchase_value'].mean().reset_index(name='avg_purchase_value')\n",
    "fraud_df = fraud_df.merge(user_avg_purchase, on='user_id', how='left')\n",
    "\n",
    "# Calculate deviation from average purchase value\n",
    "fraud_df['purchase_value_deviation'] = fraud_df['purchase_value'] - fraud_df['avg_purchase_value']\n",
    "\n",
    "print(\"Velocity and deviation features created:\")\n",
    "print(fraud_df[['user_id', 'velocity_last_24h', 'avg_purchase_value', 'purchase_value_deviation']].head(10))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nTransaction Frequency/Velocity Features Summary:\")\n",
    "print(fraud_df[['total_transactions', 'transactions_last_24h', 'transactions_last_7d', \n",
    "                'transactions_last_30d', 'velocity_last_24h']].describe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Geolocation Features (if not done in EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IP addresses to integer format\n",
    "def ip_to_int(ip_str):\n",
    "    \"\"\"Convert IP address string to integer\"\"\"\n",
    "    try:\n",
    "        return int(ipaddress.IPv4Address(ip_str))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Convert IP addresses\n",
    "fraud_df['ip_address_int'] = fraud_df['ip_address'].apply(ip_to_int)\n",
    "ip_country_df['lower_bound_int'] = ip_country_df['lower_bound_ip_address'].apply(ip_to_int)\n",
    "ip_country_df['upper_bound_int'] = ip_country_df['upper_bound_ip_address'].apply(ip_to_int)\n",
    "\n",
    "# Merge with country mapping\n",
    "def find_country(ip_int, ip_country_df):\n",
    "    \"\"\"Find country for an IP address using range-based lookup\"\"\"\n",
    "    if pd.isna(ip_int):\n",
    "        return None\n",
    "    mask = (ip_country_df['lower_bound_int'] <= ip_int) & (ip_country_df['upper_bound_int'] >= ip_int)\n",
    "    matches = ip_country_df[mask]\n",
    "    if len(matches) > 0:\n",
    "        return matches.iloc[0]['country']\n",
    "    return None\n",
    "\n",
    "fraud_df['country'] = fraud_df['ip_address_int'].apply(lambda x: find_country(x, ip_country_df))\n",
    "\n",
    "print(f\"Matched countries for {fraud_df['country'].notna().sum()} IP addresses\")\n",
    "print(f\"Unmatched: {fraud_df['country'].isna().sum()}\")\n",
    "\n",
    "# Create fraud rate by country feature (if country is available)\n",
    "if fraud_df['country'].notna().sum() > 0:\n",
    "    country_fraud_rate = fraud_df.groupby('country')['class'].mean().reset_index(name='country_fraud_rate')\n",
    "    fraud_df = fraud_df.merge(country_fraud_rate, on='country', how='left')\n",
    "    print(\"\\nCountry fraud rate feature created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Transformation for E-commerce Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "# Select features to use\n",
    "numerical_features = ['purchase_value', 'age', 'hour_of_day', 'day_of_week', 'time_since_signup',\n",
    "                      'total_transactions', 'transactions_last_24h', 'transactions_last_7d', \n",
    "                      'transactions_last_30d', 'velocity_last_24h', 'avg_purchase_value', \n",
    "                      'purchase_value_deviation']\n",
    "\n",
    "categorical_features = ['source', 'browser', 'sex']\n",
    "\n",
    "# Add country if available\n",
    "if 'country_fraud_rate' in fraud_df.columns:\n",
    "    numerical_features.append('country_fraud_rate')\n",
    "\n",
    "# Create a copy for transformation\n",
    "fraud_df_processed = fraud_df.copy()\n",
    "\n",
    "# Handle missing values in numerical features\n",
    "for col in numerical_features:\n",
    "    if fraud_df_processed[col].isnull().sum() > 0:\n",
    "        fraud_df_processed[col].fillna(fraud_df_processed[col].median(), inplace=True)\n",
    "\n",
    "# Handle missing values in categorical features\n",
    "for col in categorical_features:\n",
    "    if fraud_df_processed[col].isnull().sum() > 0:\n",
    "        fraud_df_processed[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "print(\"Missing values handled\")\n",
    "print(fraud_df_processed[numerical_features + categorical_features].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and target\n",
    "X_fraud = fraud_df_processed[numerical_features + categorical_features].copy()\n",
    "y_fraud = fraud_df_processed['class'].copy()\n",
    "\n",
    "# Split into train and test sets BEFORE any transformation (to avoid data leakage)\n",
    "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.2, random_state=42, stratify=y_fraud\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train_fraud.shape}\")\n",
    "print(f\"Test set shape: {X_test_fraud.shape}\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(y_train_fraud.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test_fraud.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features (using StandardScaler)\n",
    "scaler_fraud = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data only\n",
    "X_train_fraud_scaled = X_train_fraud.copy()\n",
    "X_train_fraud_scaled[numerical_features] = scaler_fraud.fit_transform(X_train_fraud[numerical_features])\n",
    "\n",
    "# Transform test data using the fitted scaler\n",
    "X_test_fraud_scaled = X_test_fraud.copy()\n",
    "X_test_fraud_scaled[numerical_features] = scaler_fraud.transform(X_test_fraud[numerical_features])\n",
    "\n",
    "print(\"Numerical features scaled\")\n",
    "print(\"\\nScaled training data sample:\")\n",
    "print(X_train_fraud_scaled[numerical_features].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using One-Hot Encoding\n",
    "encoder_fraud = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit encoder on training data\n",
    "X_train_categorical_encoded = encoder_fraud.fit_transform(X_train_fraud[categorical_features])\n",
    "X_test_categorical_encoded = encoder_fraud.transform(X_test_fraud[categorical_features])\n",
    "\n",
    "# Get feature names\n",
    "categorical_feature_names = encoder_fraud.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Create DataFrames\n",
    "X_train_cat_df = pd.DataFrame(X_train_categorical_encoded, columns=categorical_feature_names, index=X_train_fraud.index)\n",
    "X_test_cat_df = pd.DataFrame(X_test_categorical_encoded, columns=categorical_feature_names, index=X_test_fraud.index)\n",
    "\n",
    "# Combine numerical and categorical features\n",
    "X_train_fraud_final = pd.concat([X_train_fraud_scaled[numerical_features], X_train_cat_df], axis=1)\n",
    "X_test_fraud_final = pd.concat([X_test_fraud_scaled[numerical_features], X_test_cat_df], axis=1)\n",
    "\n",
    "print(\"Categorical features encoded\")\n",
    "print(f\"\\nFinal feature count: {X_train_fraud_final.shape[1]}\")\n",
    "print(f\"\\nFeature names: {X_train_fraud_final.columns.tolist()}\")\n",
    "print(\"\\nFinal training data sample:\")\n",
    "print(X_train_fraud_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Handle Class Imbalance for E-commerce Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document class distribution BEFORE resampling\n",
    "print(\"Class Distribution BEFORE Resampling:\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Non-fraudulent (0): {(y_train_fraud == 0).sum():,} ({(y_train_fraud == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Fraudulent (1): {(y_train_fraud == 1).sum():,} ({(y_train_fraud == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  Imbalance Ratio: {(y_train_fraud == 0).sum() / (y_train_fraud == 1).sum():.2f}:1\")\n",
    "\n",
    "# Visualize before resampling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "before_counts = y_train_fraud.value_counts()\n",
    "axes[0].bar(['Non-Fraudulent (0)', 'Fraudulent (1)'], before_counts.values, color=['skyblue', 'coral'])\n",
    "axes[0].set_title('Class Distribution BEFORE Resampling')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[0].text(0, before_counts[0], f'{before_counts[0]:,}', ha='center', va='bottom')\n",
    "axes[0].text(1, before_counts[1], f'{before_counts[1]:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_plot(fig, 'fraud_smote_before_after_comparison.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the classes\n",
    "# SMOTE creates synthetic samples of the minority class\n",
    "print(\"\\nApplying SMOTE...\")\n",
    "\n",
    "smote_fraud = SMOTE(random_state=42, sampling_strategy=0.5)  # Balance to 50% minority class\n",
    "X_train_fraud_balanced, y_train_fraud_balanced = smote_fraud.fit_resample(X_train_fraud_final, y_train_fraud)\n",
    "\n",
    "print(f\"\\nClass Distribution AFTER SMOTE:\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Non-fraudulent (0): {(y_train_fraud_balanced == 0).sum():,} ({(y_train_fraud_balanced == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Fraudulent (1): {(y_train_fraud_balanced == 1).sum():,} ({(y_train_fraud_balanced == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  New Ratio: {(y_train_fraud_balanced == 0).sum() / (y_train_fraud_balanced == 1).sum():.2f}:1\")\n",
    "\n",
    "# Visualize after resampling\n",
    "after_counts = pd.Series(y_train_fraud_balanced).value_counts()\n",
    "axes[1].bar(['Non-Fraudulent (0)', 'Fraudulent (1)'], after_counts.values, color=['skyblue', 'coral'])\n",
    "axes[1].set_title('Class Distribution AFTER SMOTE')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].text(0, after_counts[0], f'{after_counts[0]:,}', ha='center', va='bottom')\n",
    "axes[1].text(1, after_counts[1], f'{after_counts[1]:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_plot(fig, 'fraud_smote_before_after_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Create a separate post-SMOTE only visualization\n",
    "fig_post, ax_post = plt.subplots(figsize=(8, 5))\n",
    "after_counts = pd.Series(y_train_fraud_balanced).value_counts()\n",
    "ax_post.bar(['Non-Fraudulent (0)', 'Fraudulent (1)'], after_counts.values, color=['skyblue', 'coral'])\n",
    "ax_post.set_title('Class Distribution AFTER SMOTE (E-commerce Data)')\n",
    "ax_post.set_ylabel('Count')\n",
    "ax_post.tick_params(axis='x', rotation=45)\n",
    "ax_post.text(0, after_counts[0], f'{after_counts[0]:,}', ha='center', va='bottom')\n",
    "ax_post.text(1, after_counts[1], f'{after_counts[1]:,}', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "save_plot(fig_post, 'fraud_post_smote_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining set shape after SMOTE: {X_train_fraud_balanced.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: If SMOTE is taking too long, you can generate the visualization separately\n",
    "# This cell can be run independently after SMOTE completes\n",
    "\n",
    "# Check if balanced data exists (from previous run)\n",
    "try:\n",
    "    # Try to load already processed data if SMOTE was run before\n",
    "    if 'y_train_fraud_balanced' in locals():\n",
    "        print(\"Using existing balanced data from current session\")\n",
    "        after_counts = pd.Series(y_train_fraud_balanced).value_counts()\n",
    "        \n",
    "        # Create post-SMOTE visualization\n",
    "        fig_post, ax_post = plt.subplots(figsize=(8, 5))\n",
    "        ax_post.bar(['Non-Fraudulent (0)', 'Fraudulent (1)'], after_counts.values, color=['skyblue', 'coral'])\n",
    "        ax_post.set_title('Class Distribution AFTER SMOTE (E-commerce Data)')\n",
    "        ax_post.set_ylabel('Count')\n",
    "        ax_post.tick_params(axis='x', rotation=45)\n",
    "        ax_post.text(0, after_counts[0], f'{after_counts[0]:,}', ha='center', va='bottom')\n",
    "        ax_post.text(1, after_counts[1], f'{after_counts[1]:,}', ha='center', va='bottom')\n",
    "        plt.tight_layout()\n",
    "        save_plot(fig_post, 'fraud_post_smote_distribution.png')\n",
    "        plt.show()\n",
    "        print(\"✓ Post-SMOTE visualization saved!\")\n",
    "    else:\n",
    "        print(\"⚠ Balanced data not found. Please run the SMOTE cell first (Cell 19).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure SMOTE has completed running.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed e-commerce data\n",
    "X_train_fraud_balanced_df = pd.DataFrame(X_train_fraud_balanced, columns=X_train_fraud_final.columns)\n",
    "X_test_fraud_final_df = X_test_fraud_final.copy()\n",
    "\n",
    "# Save to processed folder\n",
    "X_train_fraud_balanced_df.to_csv(PROCESSED_DIR / 'fraud_train_features.csv', index=False)\n",
    "y_train_fraud_balanced_df = pd.DataFrame(y_train_fraud_balanced, columns=['class'])\n",
    "y_train_fraud_balanced_df.to_csv(PROCESSED_DIR / 'fraud_train_target.csv', index=False)\n",
    "\n",
    "X_test_fraud_final_df.to_csv(PROCESSED_DIR / 'fraud_test_features.csv', index=False)\n",
    "y_test_fraud_df = pd.DataFrame(y_test_fraud, columns=['class'])\n",
    "y_test_fraud_df.to_csv(PROCESSED_DIR / 'fraud_test_target.csv', index=False)\n",
    "\n",
    "print(\"E-commerce fraud data saved to processed folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Credit Card Fraud Data Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credit card data\n",
    "cc_df = pd.read_csv(DATA_DIR / 'creditcard.csv')\n",
    "\n",
    "print(f\"Credit Card Data Shape: {cc_df.shape}\")\n",
    "print(f\"Columns: {cc_df.columns.tolist()}\")\n",
    "cc_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Transformation for Credit Card Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit card data already has PCA features, so we mainly need to scale\n",
    "# Separate features and target\n",
    "X_cc = cc_df.drop('Class', axis=1)\n",
    "y_cc = cc_df['Class']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_cc, X_test_cc, y_train_cc, y_test_cc = train_test_split(\n",
    "    X_cc, y_cc, test_size=0.2, random_state=42, stratify=y_cc\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train_cc.shape}\")\n",
    "print(f\"Test set shape: {X_test_cc.shape}\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(y_train_cc.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test_cc.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (PCA features are already normalized, but Time and Amount need scaling)\n",
    "scaler_cc = StandardScaler()\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_cc_scaled = pd.DataFrame(\n",
    "    scaler_cc.fit_transform(X_train_cc),\n",
    "    columns=X_train_cc.columns,\n",
    "    index=X_train_cc.index\n",
    ")\n",
    "\n",
    "# Transform test data\n",
    "X_test_cc_scaled = pd.DataFrame(\n",
    "    scaler_cc.transform(X_test_cc),\n",
    "    columns=X_test_cc.columns,\n",
    "    index=X_test_cc.index\n",
    ")\n",
    "\n",
    "print(\"Credit card features scaled\")\n",
    "print(\"\\nScaled training data sample:\")\n",
    "print(X_train_cc_scaled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Handle Class Imbalance for Credit Card Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document class distribution BEFORE resampling\n",
    "print(\"Class Distribution BEFORE Resampling:\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Non-fraudulent (0): {(y_train_cc == 0).sum():,} ({(y_train_cc == 0).mean()*100:.4f}%)\")\n",
    "print(f\"  Fraudulent (1): {(y_train_cc == 1).sum():,} ({(y_train_cc == 1).mean()*100:.4f}%)\")\n",
    "print(f\"  Imbalance Ratio: {(y_train_cc == 0).sum() / (y_train_cc == 1).sum():.2f}:1\")\n",
    "\n",
    "# Visualize before resampling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "before_counts_cc = y_train_cc.value_counts()\n",
    "axes[0].bar(['Non-Fraudulent (0)', 'Fraudulent (1)'], before_counts_cc.values, color=['skyblue', 'coral'])\n",
    "axes[0].set_title('Class Distribution BEFORE Resampling')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[0].text(0, before_counts_cc[0], f'{before_counts_cc[0]:,}', ha='center', va='bottom')\n",
    "axes[0].text(1, before_counts_cc[1], f'{before_counts_cc[1]:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: If SMOTE is taking too long, you can generate the visualization separately\n",
    "# This cell can be run independently after SMOTE completes\n",
    "\n",
    "# Check if balanced data exists (from previous run)\n",
    "try:\n",
    "    # Try to load already processed data if SMOTE was run before\n",
    "    if 'y_train_cc_balanced' in locals():\n",
    "        print(\"Using existing balanced data from current session\")\n",
    "        after_counts_cc = pd.Series(y_train_cc_balanced).value_counts()\n",
    "        \n",
    "        # Create post-SMOTE visualization\n",
    "        fig_post_cc, ax_post_cc = plt.subplots(figsize=(8, 5))\n",
    "        ax_post_cc.bar(['Non-Fraudulent (0)', 'Fraudulent (1)'], after_counts_cc.values, color=['skyblue', 'coral'])\n",
    "        ax_post_cc.set_title('Class Distribution AFTER SMOTE (Credit Card Data)')\n",
    "        ax_post_cc.set_ylabel('Count')\n",
    "        ax_post_cc.tick_params(axis='x', rotation=45)\n",
    "        ax_post_cc.text(0, after_counts_cc[0], f'{after_counts_cc[0]:,}', ha='center', va='bottom')\n",
    "        ax_post_cc.text(1, after_counts_cc[1], f'{after_counts_cc[1]:,}', ha='center', va='bottom')\n",
    "        plt.tight_layout()\n",
    "        save_plot(fig_post_cc, 'creditcard_post_smote_distribution.png')\n",
    "        plt.show()\n",
    "        print(\"✓ Post-SMOTE visualization saved!\")\n",
    "    else:\n",
    "        print(\"⚠ Balanced data not found. Please run the SMOTE cell first (Cell 28).\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure SMOTE has completed running.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE for credit card data\n",
    "# Note: For extremely imbalanced data, we might use a lower sampling_strategy\n",
    "print(\"\\nApplying SMOTE...\")\n",
    "\n",
    "smote_cc = SMOTE(random_state=42, sampling_strategy=0.1)  # Balance to 10% minority class (less aggressive)\n",
    "X_train_cc_balanced, y_train_cc_balanced = smote_cc.fit_resample(X_train_cc_scaled, y_train_cc)\n",
    "\n",
    "print(f\"\\nClass Distribution AFTER SMOTE:\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Non-fraudulent (0): {(y_train_cc_balanced == 0).sum():,} ({(y_train_cc_balanced == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Fraudulent (1): {(y_train_cc_balanced == 1).sum():,} ({(y_train_cc_balanced == 1).mean()*100:.2f}%)\")\n",
    "print(f\"  New Ratio: {(y_train_cc_balanced == 0).sum() / (y_train_cc_balanced == 1).sum():.2f}:1\")\n",
    "\n",
    "# Visualize after resampling\n",
    "after_counts_cc = pd.Series(y_train_cc_balanced).value_counts()\n",
    "axes[1].bar(['Non-Fraudulent (0)', 'Fraudulent (1)'], after_counts_cc.values, color=['skyblue', 'coral'])\n",
    "axes[1].set_title('Class Distribution AFTER SMOTE')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].text(0, after_counts_cc[0], f'{after_counts_cc[0]:,}', ha='center', va='bottom')\n",
    "axes[1].text(1, after_counts_cc[1], f'{after_counts_cc[1]:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_plot(fig, 'creditcard_smote_before_after_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Create a separate post-SMOTE only visualization\n",
    "fig_post_cc, ax_post_cc = plt.subplots(figsize=(8, 5))\n",
    "after_counts_cc = pd.Series(y_train_cc_balanced).value_counts()\n",
    "ax_post_cc.bar(['Non-Fraudulent (0)', 'Fraudulent (1)'], after_counts_cc.values, color=['skyblue', 'coral'])\n",
    "ax_post_cc.set_title('Class Distribution AFTER SMOTE (Credit Card Data)')\n",
    "ax_post_cc.set_ylabel('Count')\n",
    "ax_post_cc.tick_params(axis='x', rotation=45)\n",
    "ax_post_cc.text(0, after_counts_cc[0], f'{after_counts_cc[0]:,}', ha='center', va='bottom')\n",
    "ax_post_cc.text(1, after_counts_cc[1], f'{after_counts_cc[1]:,}', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "save_plot(fig_post_cc, 'creditcard_post_smote_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining set shape after SMOTE: {X_train_cc_balanced.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed credit card data\n",
    "X_train_cc_balanced_df = pd.DataFrame(X_train_cc_balanced, columns=X_train_cc_scaled.columns)\n",
    "X_test_cc_scaled_df = X_test_cc_scaled.copy()\n",
    "\n",
    "# Save to processed folder\n",
    "X_train_cc_balanced_df.to_csv(PROCESSED_DIR / 'creditcard_train_features.csv', index=False)\n",
    "y_train_cc_balanced_df = pd.DataFrame(y_train_cc_balanced, columns=['Class'])\n",
    "y_train_cc_balanced_df.to_csv(PROCESSED_DIR / 'creditcard_train_target.csv', index=False)\n",
    "\n",
    "X_test_cc_scaled_df.to_csv(PROCESSED_DIR / 'creditcard_test_features.csv', index=False)\n",
    "y_test_cc_df = pd.DataFrame(y_test_cc, columns=['Class'])\n",
    "y_test_cc_df.to_csv(PROCESSED_DIR / 'creditcard_test_target.csv', index=False)\n",
    "\n",
    "print(\"Credit card data saved to processed folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### E-commerce Fraud Data:\n",
    "1. **Features Created:**\n",
    "   - Time-based: hour_of_day, day_of_week, time_since_signup\n",
    "   - Transaction frequency: total_transactions, transactions_last_24h/7d/30d\n",
    "   - Transaction velocity: velocity_last_24h\n",
    "   - Purchase patterns: avg_purchase_value, purchase_value_deviation\n",
    "   - Geolocation: country, country_fraud_rate\n",
    "\n",
    "2. **Data Transformation:**\n",
    "   - Numerical features: StandardScaler\n",
    "   - Categorical features: One-Hot Encoding\n",
    "\n",
    "3. **Class Imbalance Handling:**\n",
    "   - Method: SMOTE (sampling_strategy=0.5)\n",
    "   - Before: [Document ratio]\n",
    "   - After: [Document ratio]\n",
    "\n",
    "### Credit Card Fraud Data:\n",
    "1. **Data Transformation:**\n",
    "   - All features scaled using StandardScaler\n",
    "\n",
    "2. **Class Imbalance Handling:**\n",
    "   - Method: SMOTE (sampling_strategy=0.1)\n",
    "   - Before: [Document ratio]\n",
    "   - After: [Document ratio]\n",
    "\n",
    "### Next Steps:\n",
    "- Model training and evaluation\n",
    "- Model selection and comparison\n",
    "- Model explainability analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
